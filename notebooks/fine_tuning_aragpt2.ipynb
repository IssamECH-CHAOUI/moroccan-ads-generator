{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lib"
      ],
      "metadata": {
        "id": "18FA3D4A9vWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "xTWlFiZRMojV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa28dca6-6a3a-4502-d397-0bb0df60e0d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-Y_yHh9U3Ik5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_arabic_text(text):\n",
        "    # Remove emojis and latin text\n",
        "    text = re.sub(r\"[^\\u0600-\\u06FF\\s]+\", \"\", text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    # Return the remaining Arabic text\n",
        "    return text.strip()\n",
        "\n",
        "# Example usage\n",
        "text = \"Hello 👋, مرحبا بالعالم\"\n",
        "arabic_text = extract_arabic_text(text)\n",
        "print(arabic_text)  # Output: \"مرحبا بالعالم\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN5Voy1I38Vh",
        "outputId": "5f12f9af-2fa3-49ad-9b08-35c0ee73f7f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "مرحبا بالعالم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "44hGsiAp95_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the training data\n",
        "with open(\"/content/drive/MyDrive/ads.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "GnB1CTctOq1p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the training data\n",
        "with open(\"/content/drive/MyDrive/arabic_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "IsnJAFWl-aEH"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "databn = pd.read_csv(\"/content/drive/MyDrive/MTCD.csv\")"
      ],
      "metadata": {
        "id": "0VZxvC3729c0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "j6KyVeIr-fA8",
        "outputId": "6270b1d9-2e5b-4d36-adab-da6ac7d6b210"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffسلام لباس كدير بخير\\nأجي تشرب قهوة\\nسر تلعب'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOX-E1_33MrX",
        "outputId": "14b6965f-c9e0-4ba7-e2a6-347523747657"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30051"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "databn = pd.read_csv(\"/content/drive/MyDrive/MTCD.csv\")\n",
        "databn[\"text\"] = databn[\"text\"].apply(extract_arabic_text)\n",
        "databn = databn.drop(columns= \"labels\")"
      ],
      "metadata": {
        "id": "f4krJgzG4BSR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataa = pd.read_csv(\"/content/drive/MyDrive/ads.csv\", encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "h3KCjMgF_rYH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataa[:30]"
      ],
      "metadata": {
        "id": "r4qyJd5cANOg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "_rtsgkDsJVbE",
        "outputId": "11136592-f945-4de6-d928-3b1d12bac944"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            ads_clean\n",
              "0   حصرياً و غير عند اورنج، عيش لفيبر فدارك ونتا ه...\n",
              "1   تعرفوا معنا على سر الحرفة مع فاطمة، عمران و هش...\n",
              "2   بفضل الخطوات ديالكم وزعنا الاأنترنت على عدة جم...\n",
              "3   الساعة جديدة هادي ولا القديمة ؟ شكون فعائلتك و...\n",
              "4   حيت عندنا ديما نتا لول، تبرع بالماكس ديال السخ...\n",
              "5   اتبعوا معنا تجربة المقاول عمران سينو رئيس تعاو...\n",
              "6   ماكين غير فورفي تبرع باللامحدود، تمزك، تفرج و ...\n",
              "7   شارك ف مع أورنج و جمع الخطوات ديالك باش تساهم ...\n",
              "8   أورنج، ريزو لي فين ماكنتي يجري بيك للقدام الري...\n",
              "9   غير عند أورنج هاد رمضان ماكين غير لفراجة قسيمة...\n",
              "10  بفضل الخطوات ديالكم جمعنا الأنترنيت لي تم توزي...\n",
              "11  بفضل الخطوات ديالكم جمعنا الأنترنيت لي تم توزي...\n",
              "12  اتبعوا معنا تجربة المقاولة بايا فاطمة صاحبة مش...\n",
              "13  خطوة خير مازال مستمرة هاد رمضان على كل خطوة در...\n",
              "14  هاد رمضان تفرج و زييييد تفرج في آخر الأفلام و ...\n",
              "15  حيت عارفينكم كتبغيو تفرجو من التيران، أورنج جا...\n",
              "16  حيت عند أورنج نتوما المشجعين رقم أورنج جات حتا...\n",
              "17  اتبعوا معنا تجربة المقاول هشام بلعزري صاحب مشر...\n",
              "18  بغيتو فرحة رمضان تكمل و تستمتعوا فكل نهار فيه ...\n",
              "19  الخير غير بخطوة كافي، كنتي شيخ ولا صابي، حيت غ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-126364d2-ad92-491e-b397-fbe4568a022f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ads_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>حصرياً و غير عند اورنج، عيش لفيبر فدارك ونتا ه...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>تعرفوا معنا على سر الحرفة مع فاطمة، عمران و هش...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>بفضل الخطوات ديالكم وزعنا الاأنترنت على عدة جم...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>الساعة جديدة هادي ولا القديمة ؟ شكون فعائلتك و...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>حيت عندنا ديما نتا لول، تبرع بالماكس ديال السخ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>اتبعوا معنا تجربة المقاول عمران سينو رئيس تعاو...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ماكين غير فورفي تبرع باللامحدود، تمزك، تفرج و ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>شارك ف مع أورنج و جمع الخطوات ديالك باش تساهم ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>أورنج، ريزو لي فين ماكنتي يجري بيك للقدام الري...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>غير عند أورنج هاد رمضان ماكين غير لفراجة قسيمة...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>بفضل الخطوات ديالكم جمعنا الأنترنيت لي تم توزي...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>بفضل الخطوات ديالكم جمعنا الأنترنيت لي تم توزي...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>اتبعوا معنا تجربة المقاولة بايا فاطمة صاحبة مش...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>خطوة خير مازال مستمرة هاد رمضان على كل خطوة در...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>هاد رمضان تفرج و زييييد تفرج في آخر الأفلام و ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>حيت عارفينكم كتبغيو تفرجو من التيران، أورنج جا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>حيت عند أورنج نتوما المشجعين رقم أورنج جات حتا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>اتبعوا معنا تجربة المقاول هشام بلعزري صاحب مشر...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>بغيتو فرحة رمضان تكمل و تستمتعوا فكل نهار فيه ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>الخير غير بخطوة كافي، كنتي شيخ ولا صابي، حيت غ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-126364d2-ad92-491e-b397-fbe4568a022f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-126364d2-ad92-491e-b397-fbe4568a022f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-126364d2-ad92-491e-b397-fbe4568a022f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelWithLMHead\n",
        "import pandas as pd\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer for Arabic\n",
        "model_name = \"aubmindlab/aragpt2-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelWithLMHead.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Preprocess the data\n",
        "max_length = 512\n",
        "input_ids = []\n",
        "for text in data[\"ads_clean\"]:\n",
        "    encoded_text = tokenizer.encode(text, add_special_tokens=True, max_length=max_length)\n",
        "    input_ids.append(encoded_text)\n",
        "input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=max_length, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Define the training parameters\n",
        "batch_size = 4\n",
        "learning_rate = 5e-5\n",
        "num_epochs = 6\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Define the training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for i in range(0, len(input_ids), batch_size):\n",
        "        batch = input_ids[i:i+batch_size]\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(batch, return_dict=True)\n",
        "            loss_value = loss(batch[:, 1:], outputs.logits[:, :-1, :])\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        print(f\"Step {i//batch_size+1}/{len(input_ids)//batch_size+1}, Loss: {loss_value.numpy():.4f}\")\n",
        "    \n",
        "    # Generate text after each epoch\n",
        "    prompt = \"حيت عند أورنج نتوما\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
        "    generated = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=50,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True,\n",
        "        temperature=1.0,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "    print(f\"Generated text: {text}\")\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"arabic_gpt2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUtlxgi7_ecg",
        "outputId": "ac0c6909-7028-4a51-ca62-2ec5f9cc1aad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_tf_auto.py:649: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at aubmindlab/aragpt2-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f278001f640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f278001f640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/8, Loss: 0.5983\n",
            "Step 2/8, Loss: 0.5320\n",
            "Step 3/8, Loss: 0.4973\n",
            "Step 4/8, Loss: 0.3482\n",
            "Step 5/8, Loss: 0.4478\n",
            "Step 6/8, Loss: 0.4164\n",
            "Step 7/8, Loss: 0.2110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 8/8, Loss: 0.4326\n",
            "Generated text: حيت عند أورنج نتوما بغيتي نديرو على هاد الهضرة\n",
            "Epoch 2/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/1, Loss: 3.3114\n",
            "Generated text: حيت عند أورنج نتوما حتال على هاد الهضرة.\n",
            "Epoch 3/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/1, Loss: 1.3382\n",
            "Generated text: حيت عند أورنج نتوما لقدامو نجيوكم معنا على الفيسبوك [رابط]\n",
            "Epoch 4/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/1, Loss: 0.0636\n",
            "Generated text: حيت عند أورنج نتوما في\n",
            "Epoch 5/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/1, Loss: 0.0084\n",
            "Generated text: حيت عند أورنج نتوما في\n",
            "Epoch 6/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1/1, Loss: 0.0004\n",
            "Generated text: حيت عند أورنج نتوما كتوما إلعبوا أورسكوا معنا على اليوتوب و غادي نتواصل معكم على الفيسبوك\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelWithLMHead\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer for Arabic\n",
        "model_name = \"aubmindlab/aragpt2-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelWithLMHead.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the text\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"tf\")\n",
        "\n",
        "# Define the training parameters\n",
        "batch_size = 4\n",
        "learning_rate = 5e-5\n",
        "num_epochs = 5\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Define the training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for i in range(0, len(input_ids), batch_size):\n",
        "        batch = input_ids[i:i+batch_size]\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(batch, return_dict=True)\n",
        "            loss_value = loss(batch[:, 1:], outputs.logits[:, :-1, :])\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        print(f\"Step {i//batch_size+1}/{len(input_ids)//batch_size+1}, Loss: {loss_value.numpy():.4f}\")\n",
        "    \n",
        "    # Generate text after each epoch\n",
        "    prompt = \"اتاي\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
        "    generated = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=50,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True,\n",
        "        temperature=1.0,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "    print(f\"Generated text: {text}\")\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"arabic_gpt2\")\n"
      ],
      "metadata": {
        "id": "yf49cqunQu8Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "3c9d3966-0b9c-4245-d4e2-8a136ce90d9e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_tf_auto.py:649: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at aubmindlab/aragpt2-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-a97c0a29717a>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the text data from a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/path/to/data.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/data.txt'"
          ]
        }
      ]
    }
  ]
}